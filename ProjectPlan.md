Overview:

For this project, we will study how US interest rates relate to stock market performance, with an emphasis on the S&P 500. We plan to use historical Federal Reserve rate series and S&P 500 prices to quantify correlations between rate levels/changes and monthly index return, and how rate regimes and yield-curve slope conditions are associated with market returns and volatility.

We will include a reproducible pipeline that takes both datasets, cleans and aggregates them to a common monthly frequency, produces tables and visuals, and computes features such as returns, volatility, rate changes, etc. By doing this, we will address lifecycle, integration, cleaning, automation, and reproducibility expectations for the course this semester.

Research Question:

What has been the relationship between monthly S&P 500 returns and the federal interest rates over the past 70 years? How do changes in interest rates impact stock market/S&P 500 volatility?

Team Responsibilities:

Yash Singh (Data Prep): Will take the lead on preparing and processing the data pipeline by pulling in both datasets, cleaning/organizing them, and making sure the monthly data lines up correctly, and conduct feature analysis. Feature Analysis Will calculate monthly returns for the S&P 500, measure volatility, and track rate changes over time. Will set up the reproducibility workflow to make sure everything can be re-run easily.

Vishv Patel (Analysis/Visualization): Will take the lead on analysis and visualizations for the project. This includes taking the cleaned data to explore trends, run correlation tests, and create charts to show how interest rates and stock returns move together or apart. Potential Visualizations Scatter Plots/Correlation Charts

Together (Interpretation): Decide the best statistical approaches to use and interpret the results We will both contribute to GitHub. Maintaining an equal distribution of work as we push and work on the project together

Datasets:

Dataset 1 (S&P 500 Historical Data): The first dataset that we have is the S&P 500 historical data which provides data on the S&P 500’s opening, closing, low, and high price every trading day since 1928.

Dataset 2 (Federal Reserve Interest Rates, 1954-Present): The second dataset that we have is a dataset presenting interest rate data since 1954. This data is presented on a monthly basis and provides information on the effective interest rate, inflation rate, and unemployment rate over the months and years.

Timeline:

Week 7-8: Data Collection and Ethical Considerations During these weeks, we will download both datasets and complete set-up with our new file in GitHub. At this point, our goal is to organize the raw files and make sure everything is set to dive into the analysis We’ll make notes about any ethical or licensing issues, however, we expect these to be minimal as both of our datasets are public. Team Member Specific: Yash and Vishv will create the GitHub repository and organize raw files and confirm everything is complete and ready to use. Week 9: Organizing/Exploring Data This week, we’ll focus on cleaning the datasets and aligning them to a consistent format. This will include standardizing columns, checking for missing or duplicated values and setting up folders for raw, processed, and results data. We will conduct some exploration to see what the data looks like and gain an understanding of issues we may deal with later. Team Member Specific: Yash will standardize columns, data formats, and check for missing/duplicate values. Vishv will set up folders and run exploratory checks to spot early issues. Week 10: Creating Key Features This week, we’ll turn the raw data into something that connects to our research questions. For the S&P 500, we’ll aggregate daily prices into monthly returns and calculate volatility measures. We’ll look at rate levels and month-to-month charges with the federal data. These variables will help us test the relationship between interest rates, returns, and volatility. Team Member Specific: Yash will aggregate daily S&P 500 prices into monthly returns and calculate volatility measures. Vishv will focus on the Federal Reserve data to calculate changes in rates and preparing those features. Week 11: Merging Datasets and Checking Quality This week, we’ll merge the interest rate data with the S&P 500 data on a monthly basis after building those features. We’ll identify that time periods are aligned and we’re not missing any months. We’ll flag any odd data metrics, such as larger swings during financial crises. After that, we’ll decide whether they represent real events to keep or discrepancies we need to adjust for. Team Member Specific: Yash will merge both datasets on a monthly basis and confirm alignment. Vishv will review the merged dataset for disparities, flag outliers, and decide how to treat them. Week 12: Cleaning & Automating Process At this point, we should have a good integrated dataset and plan to polish it. We’ll handle any remaining missing values or alignment issues, and save a clean analysis-ready version. We will start building an automated workflow where our cleaning/integration steps can be produced easily from new files. Team Member Specific: Yash will finalize the cleaned dataset. Vishv will start building the automated workflow. Week 13: Analysis and Visualization With cleaned data, we will begin our actual analysis. This will include running tests on how interest rate levels line up with returns, how changes in rates affect volatility during specific time periods, and creating visuals to make the relationships easier to illustrate. Team Member Specific: Yash will design/create visuals. Vishv will run tests on relationships for outputs. Week 14: Documenting and Project Reproducibility This week, we will focus on making sure someone else can run our project from start to finish. We plan to write a data dictionary with all variables, add metadata describing the project, and test our scripts to confirm they work without any manual fixes. We will update our GitHub repository with clear documentation so it’s easy to follow. Team Member Specific: Yash will create the data dictionary and metadata. Vishv will test scripts and workflows. Week 15: Wrap-Up In the last week, we will polish our analysis, finalize the visuals, and put the report together. We’ll make sure everything is cited properly, that our repository is clean/organized, and that all requirements for the project have been addressed. We will tag the release of GitHub and submit the project. Team Member Specific: Yash will finalize citations and references. Viishv will double-check the repository is clean/organized and meets requirements. We both will polish the written report.

Constraints:

Data for the S&P 500 dataset is collected on a daily basis in comparison to the interest rate data set where data is collected on a monthly basis. That will need to be noted and we will likely have to either take the month’s average or match the data when using the S&P dataset. The stock market is volatile based on numerous metrics so we can’t assume that it is a direct cause even if a trend is seen. Rather we will only look at correlations between the datasets.

Gaps:

We have additional variables of unemployment rates and inflation rates on a monthly basis as well. We aren’t using these in our project at the moment since they are out of scope of our research question, but could potentially use them later if cases for them arise. We have not yet finalized which methods of statistical tests to use and which visualizations to have to showcase out findings. This will likely be finalized as we dive deeper into the project and see which one we find to be the best potential choice.
